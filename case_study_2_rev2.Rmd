---
title: "case_study_2"
author: "Travis Deason"
date: "12/4/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, message=FALSE}
rm( list = ls()); cat("\014")  # Clear environment
#install.packages('tidyr')
#install.packages('dplyr')
#install.packages('ggplot2')
#install.packages('Hmisc')
#install.packages('plotly')
library(plotly)
library(Hmisc)
library(tidyr)
library(dplyr)
library(ggplot2)
data <- read.csv('data/CaseStudy2-data.csv')
describe_data <- read.csv('data/CaseStudy2-descriptions.csv')
#names(describe_data) <- (names(describe_data), tolower)
names(data) <- sapply(names(data), tolower)
```
## Project Goals

* Identify (at least) the top three factors that contribute to turnover.

* Learn about any job role specific trends that may exist in the data set

* Provide any other interesting trends and observations from your analysis

```{r echo=TRUE }
## this doesn't really seem needed yet... just know lower is better
labels <- sapply(unique(describe_data$X), tolower)
describe_clean <- fill(describe_data, X, .direction='up')
dump <- describe_data[6,1]
is(dump)
```

```{r echo=TRUE}
str(data)
```

* Because we have a mix of catagoriacal and numerical data in here, I am going to split every integer column into binned values

```{r}
types <- sapply(data, class)
data_binned <- data
for(col in names(types)){
  if(types[[col]] == 'integer' & length(unique(data[,col])) > 10){
    data_binned[col] <- cut2(data$age, m=100, g=10)
  }
}
```

* Now, lets find out the base attrition rate

```{r}
na_count <- sum(is.na(data$attrition))
## convert data Attrition to binary
data_binned$attrition <- data$attrition == 'Yes'
## N/A Count
print(sum(na_count))
## Sum of Employees who quit
print(sum(data_binned$attrition))
## retain ratio
<<<<<<< HEAD
overall_ratio <- sum(data_binned$attrition) / length(data_binned$attrition)
=======
overall_ratio <- sum(na_count) / length(data$attrition)
>>>>>>> 3edf48fe69d53af1d011f874a5cfa1d26f3e0a94
print(overall_ratio)
```



* So, there are no N/A values in attrition
* 237 total employees quit out of 1470 total (which is 16.122 percent)


* Now (Since we have binned all continous values) lets treat all values as catagorical, and find out which of them have a higher then expected corelation with

* First we will create a vector which contains the lengths of all catagories, or factors

```{r}
num_obs= c()
for (col in names(data_binned))
 ## this is not needed since there are no N/A values in the dataset
 # unique_vals[paste(col, 'isna', sep='#')] = sum(is.na(data[,col])) / 1470
  {for (value in unique(data_binned[,col]))
    {
    num_obs[paste(col, value, sep='&')] = sum(data_binned[,col] == value)
    }
}
print(mean(num_obs))

obs_stacked = stack(num_obs)
ggplot(obs_stacked, aes(x = values)) +
  stat_density(position="identity",geom="line")
```

*It looks like we have 216 possible factors with an mean of 238 observations each, and most factors seem to have around ~170 variables, so we are fairly well distributed, but it would be nice if one of those factors with 1000+ had a zero attrition rate

```{r}
## first we will define a function to build a new data.frame which contains qutting percent and number of observations for each factor
percent_quit = c()
find_percent <- function(df, sub_df, col_value, num_observations)
  {
  colval <- unlist(strsplit(col_value, '&'))
  return(sum(sub_df[,colval[1]] == colval[2])  / num_observations)
}

## Then we can iterate over all our factors and extract the data we are looking for from them
percent_quit = c()
sub_df <- subset(data_binned, attrition == TRUE)
for (colval in names(num_obs)){
  percent_quit[colval] = find_percent(data_binned, sub_df, colval, num_obs[colval])}
```


```{r}
print(mean(percent_quit))
print(median(percent_quit))
per_stacked = stack(percent_quit)
ggplot(per_stacked, aes(x = values)) +
  stat_density(position="identity",geom="line")
```

* Unsuprisingly, our mean value for percent_quit is exactly in line with the overall percent who quit, but the median value is notably lower, and we seem to have a small cluster of events hovering around .33% quit; which is approximitly double the standard rate

* Let's take a closer look at some of those varaibles

```{r}
## move all our data into a dataframe of percent_quit verses number of observations
attr_frame <- data.frame(percent_quit, num_obs)
high_prob <- subset(attr_frame, percent_quit > overall_ratio)
high_prob$ratio_delta = high_prob$percent_quit - overall_ratio
hnames <- rownames(high_prob) != 'attrition&TRUE'
high_prob <- high_prob[hnames,]
head(high_prob[order(-high_prob$ratio_delta),], 40)
```


```{r}
hist(data$percentsalaryhike)
hist(data$numcompaniesworked)
hist(data$environmentsatisfaction)
hist(data$totalworkingyears)
```


* Taking care of all the catagorical variables

```{r}
print(unique(data$businesstravel)) # sequential data, can be assigned nums
print(unique(data$department)) ## Dummy for:
## Sales                  Research & Development Human Resources       
<<<<<<< HEAD
## Human Resources Research & Development Sales
=======
# Human Resources Research & Development Sales
>>>>>>> 3edf48fe69d53af1d011f874a5cfa1d26f3e0a94
##
print(unique(data$educationfield)) ## Dummy for:
## Life Sciences    Other            Medical          Marketing        Technical Degree
## Default: Human Resources
print(unique(data$gender)) ## Dummy value for is_female
print(unique(data$jobrole)) ## Dummy for:
## Sales Executive           Research Scientist        Laboratory Technician  
## Manufacturing Director    Healthcare Representative Manager      
## Sales Representative      Research Director
## Default: Human Resources
print(unique(data$maritalstatus)) # dummy for married or divorced; single=default
print(unique(data$over18)) # there is only one level, omit this variable
print(unique(data$overtime)) #create dummy variable for yes overtime
```

<<<<<<< HEAD
=======
```{r}
#check_corr <- function(col, label)
```
>>>>>>> 3edf48fe69d53af1d011f874a5cfa1d26f3e0a94

